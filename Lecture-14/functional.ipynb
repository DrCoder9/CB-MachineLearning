{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load(\"../datasets/mnist_train_small.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[:, 1:], mnist[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hot = ohe.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer = Input((784,))\n",
    "d1 = Dense(400, activation=\"tanh\")(in_layer)\n",
    "d2 = Dense(200, activation=\"tanh\")(d1)\n",
    "d3 = Dense(50, activation=\"tanh\")(d2)\n",
    "out = Dense(10, activation=\"softmax\")(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[in_layer], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               314000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 404,760\n",
      "Trainable params: 404,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     X, y_hot, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13399/13399 [==============================] - 5s 366us/step - loss: 0.5842 - accuracy: 0.8242\n",
      "Epoch 2/10\n",
      "13399/13399 [==============================] - 4s 295us/step - loss: 0.4329 - accuracy: 0.8625\n",
      "Epoch 3/10\n",
      "13399/13399 [==============================] - 4s 293us/step - loss: 0.4266 - accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "13399/13399 [==============================] - 4s 284us/step - loss: 0.4137 - accuracy: 0.8680\n",
      "Epoch 5/10\n",
      "13399/13399 [==============================] - 4s 297us/step - loss: 0.3712 - accuracy: 0.8777\n",
      "Epoch 6/10\n",
      "13399/13399 [==============================] - 4s 300us/step - loss: 0.3507 - accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "13399/13399 [==============================] - 4s 288us/step - loss: 0.3427 - accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "13399/13399 [==============================] - 4s 291us/step - loss: 0.3541 - accuracy: 0.8861\n",
      "Epoch 9/10\n",
      "13399/13399 [==============================] - 4s 290us/step - loss: 0.3387 - accuracy: 0.8939\n",
      "Epoch 10/10\n",
      "13399/13399 [==============================] - 4s 287us/step - loss: 0.3327 - accuracy: 0.8924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x139866780>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6600 [==============================] - 1s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3713211933410529, 0.8807575702667236]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6600"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.6077594e-04, 2.6120294e-02, 3.0957636e-01, 3.3756623e-01,\n",
       "        1.7456218e-04, 6.9117700e-03, 1.4036736e-05, 1.6426900e-01,\n",
       "        1.4847536e-01, 6.6316044e-03],\n",
       "       [1.5130256e-04, 1.7804818e-05, 2.3802404e-05, 3.5996042e-04,\n",
       "        4.0820029e-04, 1.7390089e-04, 7.3684492e-07, 9.7877216e-01,\n",
       "        1.1594123e-04, 1.9976102e-02],\n",
       "       [1.7907061e-01, 2.5308146e-03, 4.9993330e-01, 7.8136832e-02,\n",
       "        2.4943228e-04, 5.8371476e-03, 6.2201649e-04, 5.5412997e-02,\n",
       "        1.2761076e-01, 5.0596107e-02],\n",
       "       [7.0287366e-05, 5.1305917e-05, 8.3354671e-06, 6.9733374e-03,\n",
       "        1.2582287e-02, 1.0073536e-02, 3.3137123e-05, 1.7519519e-03,\n",
       "        2.4108437e-03, 9.6604508e-01],\n",
       "       [1.3811431e-04, 2.4078253e-03, 1.9681091e-05, 2.3356758e-03,\n",
       "        5.8405042e-01, 4.6526622e-03, 6.7597773e-04, 1.9110559e-02,\n",
       "        2.8137023e-02, 3.5847199e-01],\n",
       "       [1.3027898e-04, 1.2783275e-04, 1.9777008e-06, 5.1624386e-04,\n",
       "        3.1013443e-04, 9.9649835e-01, 9.2029717e-05, 2.2399987e-04,\n",
       "        1.8901965e-03, 2.0896694e-04],\n",
       "       [3.2344623e-04, 1.1577063e-04, 1.1445275e-04, 6.7647401e-04,\n",
       "        3.5997200e-01, 1.4940670e-04, 2.6176354e-05, 3.3104897e-02,\n",
       "        2.5064584e-03, 6.0301095e-01],\n",
       "       [1.1462343e-03, 1.1045400e-05, 8.2478439e-04, 2.9884417e-05,\n",
       "        2.5217170e-03, 8.2079862e-04, 9.9418569e-01, 7.3601987e-05,\n",
       "        7.5962329e-05, 3.1026773e-04],\n",
       "       [1.9533101e-03, 1.3024978e-05, 3.2252306e-04, 9.8341797e-04,\n",
       "        2.4065137e-02, 8.6216460e-04, 1.7260707e-05, 4.3791956e-03,\n",
       "        5.1434040e-03, 9.6226060e-01],\n",
       "       [7.1785631e-05, 9.3177184e-03, 9.7976273e-01, 8.2934024e-03,\n",
       "        2.2326881e-06, 1.2653890e-03, 3.9045553e-04, 3.9866496e-05,\n",
       "        8.5320021e-04, 3.1623383e-06]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[7],\n",
       "        [7],\n",
       "        [0],\n",
       "        [9],\n",
       "        [4],\n",
       "        [5],\n",
       "        [4],\n",
       "        [6],\n",
       "        [9],\n",
       "        [2]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10].todense().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8434976b1772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/nums/eight.jpeg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplane\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "img = load_img(\"../datasets/nums/eight.jpeg\", target_size=(28, 28), grayscale=True)\n",
    "type(img)\n",
    "plane = (np.array(img) < 100).astype(int) * 255\n",
    "plt.imshow(plane, cmap=\"gray\")\n",
    "model.predict(np.array([plane.flatten()])).argsort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
